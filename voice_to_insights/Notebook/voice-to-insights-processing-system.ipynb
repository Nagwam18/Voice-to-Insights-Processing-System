{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14116963,"sourceType":"datasetVersion","datasetId":8993147},{"sourceId":14117601,"sourceType":"datasetVersion","datasetId":8993624},{"sourceId":14152484,"sourceType":"datasetVersion","datasetId":9020167},{"sourceId":14248174,"sourceType":"datasetVersion","datasetId":9090716},{"sourceId":14248601,"sourceType":"datasetVersion","datasetId":9091044}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Voice-to-Insights AI system \n\n * that transforms audio files into structured, actionable insights using **Fast Whisper**, **LLMs**,\n    **Sentiment Analysis**, **FastAPI REST API**, and **Dash**\n    \n* **You Can Check Project GitHub Repo From [this link](https://github.com/Nagwam18/Voice-to-Insights-Processing-System/tree/main)**","metadata":{}},{"cell_type":"markdown","source":"# Installation","metadata":{}},{"cell_type":"code","source":"# requirements.txt\n# Core LLM dependencies\n# transformers==4.45.2\n# accelerate==0.34.2\n# safetensors==0.4.3\n# sentencepiece\n\n# # PyTorch + CUDA 11.8 (for Kaggle & Llama)\n# torch==2.3.1+cu118\n# torchvision==0.18.1+cu118\n# torchaudio==2.3.1+cu118\n# --extra-index-url https://download.pytorch.org/whl/cu118\n\n# # # Optional dependencies\n# av\n# # Core LLM dependencies\n# transformers==4.45.2\n# accelerate==0.34.2\n# safetensors==0.4.3\n# sentencepiece\n\n# # PyTorch + CUDA 11.8 (for Kaggle & Llama)\n# torch==2.3.1+cu118\n# torchvision==0.18.1+cu118\n# torchaudio==2.3.1+cu118\n# -f https://download.pytorch.org/whl/cu118\n\n# # Optional dependencies\n# av\n# protobuf==5.26.1\n\n!pip install -r /kaggle/input/install-2/requirements.txt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pydantic-ai\n!pip install huggingface_hub\n!pip install pyngrok\n!pip install dash dash-bootstrap-components requests pydub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# Core utilities\nimport re\nimport json\nfrom threading import Thread\n\n# Machine learning & NLP libraries\nimport torch\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\n# Whisper speech-to-text\nfrom faster_whisper import WhisperModel\n\n# FastAPI server\nfrom fastapi import FastAPI\nfrom fastapi.responses import JSONResponse\nimport uuid\nimport uvicorn\nimport nest_asyncio\nimport requests\n\n#pydantic set-up\nfrom typing import List, Optional\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent\nfrom typing import List\nfrom transformers import pipeline\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# login to Hugging Face","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"HF_TOKEN\"] = \"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\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_transcription(file_path):\n    model_size = \"large-v2\"\n    model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n    segments, _ = model.transcribe(file_path, beam_size=5)\n    \n    transcription = \"\".join(segment.text for segment in segments)\n    return transcription.strip()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# voice_path= \"/kaggle/input/test-voice/test1.mp3\"\n# voice_path=\"/kaggle/input/test-voice/test2.mp3\"\n# voice_path=\"/kaggle/input/test-voice/test3.mp3\"\n# voice_path=\"/kaggle/input/test2-voice/test4.mp3\"\n# voice_path=\"/kaggle/input/test3-voice/test5.mp3\"\nvoice_path=\"/kaggle/input/test3-voice/test6.mp3\"\n\n# print(\"\\nTranscript\")\ntranscript = get_transcription(voice_path)\nprint(transcript)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Insights(BaseModel):\n    summary: str = Field(description=\"Concise summary of the input text in 2–3 sentences\")\n    \n    entities: List[str] = Field(description=\"Important names, identifiers, objects, or key terms mentioned\")\n    \n    actions: List[str] = Field(description=\"Explicit or implicit actions, requests, or next steps described\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PROMPT = \"\"\"\nYou are a universal information extraction engine.\n\nAnalyze the input text regardless of domain (technical, business, casual, medical, legal, etc.).\nYour task is to extract structured insights that strictly follow the provided schema.\n\nRules:\n1. Return ONLY valid JSON that matches the schema exactly.\n2. Do NOT add extra fields or explanations.\n3. 'entities' must be concrete and specific (IDs, names, objects, issues).\n4. 'actions' must describe what happened or what should happen next.\n5. Infer implicit actions when reasonable.\n\nBe concise, accurate, and deterministic.\n\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"agent = Agent(\n    model=HuggingFaceModel(\"Qwen/Qwen2.5-7B-Instruct\"),\n    output_type=Insights,\n    system_prompt=PROMPT\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"async def analyze_text(text: str) -> dict:\n    result = await agent.run(text)   \n    insight: Insights = result.output\n    return insight.model_dump()\n\ninsight_json = await analyze_text(transcript)\nprint(insight_json)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" def sentiment_analysis(transcript):\n    sentiment_analyzer = pipeline(\"sentiment-analysis\", device=0)\n    result = sentiment_analyzer(transcript)[0]\n    \n    # print(\"Raw sentiment:\", result)\n    \n    if result['label'] == \"NEGATIVE\":\n        sentiment = \"Frustrated but cooperative\"\n    elif result['label'] == \"POSITIVE\":\n        sentiment = \"Satisfied and cooperative\"\n    else:\n        sentiment = \"Neutral but cooperative\"\n    \n    return sentiment","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fast API APP","metadata":{}},{"cell_type":"code","source":"nest_asyncio.apply()\napp = FastAPI()\n\nresults_store = {}\n\n@app.post(\"/process_audio\")\nasync def process_audio():\n    session_id = str(uuid.uuid4())\n    \n    # voice_path= \"/kaggle/input/test-voice/test1.mp3\"\n    # voice_path=\"/kaggle/input/test-voice/test2.mp3\"\n    # voice_path=\"/kaggle/input/test-voice/test3.mp3\"\n    # voice_path=\"/kaggle/input/test2-voice/test4.mp3\"\n    # voice_path=\"/kaggle/input/test3-voice/test5.mp3\"\n    voice_path=\"/kaggle/input/test3-voice/test6.mp3\"\n\n\n\n    transcript = get_transcription(voice_path)\n    insights_json = await analyze_text(transcript)\n    sentiment_json = sentiment_analysis(transcript)\n\n\n    \n    results_store[session_id] = {\n        \"status\": \"completed\",\n        \"results\": { \"transcript\":transcript,\n                     \"insights\": insights_json,\n                     \"sentiment\": sentiment_json }\n                     }\n\n    return JSONResponse({\n        \"session_id\": session_id,\n        \"status\": \"processing\",\n        \"message\": \"Audio path received. Processing started.\"\n    })\n\n\n@app.get(\"/results/{session_id}\")\nasync def get_results(session_id: str):\n    if session_id not in results_store:\n        return JSONResponse({\"error\": \"Session not found\"}, status_code=404)\n\n    return JSONResponse({\n        \"session_id\": session_id,\n        \"results\": results_store[session_id][\"results\"],\n        \"processing_status\": results_store[session_id][\"status\"],\n\n    })\nimport nest_asyncio\nnest_asyncio.apply()\nimport time\n\ndef run_api():\n  uvicorn.run(app, host=\"0.0.0.0\", port=8003)\n\nthread = Thread(target=run_api, daemon=True)\nthread.start()\ntime.sleep(2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resp = requests.post(\"http://127.0.0.3:8003/process_audio\")\nsession_id = resp.json()[\"session_id\"]\nresp_results = requests.get(f\"http://127.0.0.3:8003/results/{session_id}\")\n\nprint(\"GET status:\", resp_results.status_code)\nprint(\"GET response:\")\nprint(json.dumps(resp_results.json(), indent=4))   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyngrok import ngrok, conf\nconf.get_default().auth_token = \"36F0BxzfgoiXChAeN7oJ4MflnlF_2AjXn8jbAnkxHQh8WLAiT\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import dash\nfrom dash import html, dcc, Input, Output, State\nimport dash_bootstrap_components as dbc\nimport base64\nimport tempfile\nimport asyncio\nfrom threading import Thread\nfrom pyngrok import ngrok\n\napp = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\nserver = app.server\napp.layout = dbc.Container([\n    html.H1(\"Voice-to-Insights Processing System\", className=\"text-center my-4\",style={\"fontSize\": \"50px\"}),\n\n    dbc.Row([\n        dbc.Col([\n            dcc.Upload(\n                id='upload-audio',\n                children=html.Div(id=\"upload-text\", children=['Drag and Drop or ', html.A('Select Audio File')]),\n                style={'width':'100%','height':'60px','lineHeight':'60px','borderWidth':'1px',\n                       'borderStyle':'dashed','borderRadius':'5px','textAlign':'center','margin-bottom':'10px'},\n                multiple=False\n            ),\n            dbc.Button(\"Process Audio\", id=\"process-btn\", color=\"primary\", n_clicks=0),\n            html.Div(id=\"loading-output\", className=\"mt-2\"),\n            html.Div(id=\"ngrok-link\", className=\"mt-2\")\n        ], width=6)\n    ], justify=\"center\"),\n\n    dbc.Row([\n        dbc.Col([\n            html.H4(\"Transcript\"),\n            dbc.Card(dbc.CardBody(id=\"transcript-output\"), className=\"mb-3\"),\n\n            html.H4(\"Summary\"),\n            dbc.Card(dbc.CardBody(id=\"summary-output\"), className=\"mb-3\"),\n        ], width=6),\n\n        dbc.Col([\n            html.H4(\"Entities\"),\n            dbc.Card(dbc.CardBody(id=\"entities-output\"), className=\"mb-3\"),\n\n            html.H4(\"Actions\"),\n            dbc.Card(dbc.CardBody(id=\"actions-output\"), className=\"mb-3\"),\n\n            html.H4(\"Sentiment\"),\n            dbc.Card(dbc.CardBody(id=\"sentiment-output\"), className=\"mb-3\"),\n        ], width=6),\n    ])\n], fluid=True)\n\n@app.callback(\n    Output(\"upload-text\", \"children\"),\n    Input(\"upload-audio\", \"contents\")\n)\ndef update_upload_text(contents):\n    if contents:\n        return \"Audio uploaded ✅\"\n    return ['Drag and Drop or ', html.A('Select Audio File')]\n\n\n@app.callback(\n    Output(\"loading-output\", \"children\"),\n    Output(\"transcript-output\", \"children\"),\n    Output(\"summary-output\", \"children\"),\n    Output(\"entities-output\", \"children\"),\n    Output(\"actions-output\", \"children\"),\n    Output(\"sentiment-output\", \"children\"),\n    Input(\"process-btn\", \"n_clicks\"),\n    State(\"upload-audio\", \"contents\")\n)\ndef process_audio_callback(n_clicks, audio_contents):\n    if n_clicks == 0 or audio_contents is None:\n        return \"\", \"\", \"\", \"\", \"\", \"\"\n\n    loading_spinner = dbc.Spinner(size=\"sm\", color=\"primary\", children=\"Processing audio...\")\n\n    header, encoded = audio_contents.split(\",\", 1)\n    audio_bytes = base64.b64decode(encoded)\n    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n    tmp_file.write(audio_bytes)\n    tmp_file.close()\n\n    transcript = get_transcription(tmp_file.name)\n    insights_json = asyncio.run(analyze_text(transcript))\n    sentiment_json = sentiment_analysis(transcript)\n\n    return (\n        \"\",  \n        transcript,\n        insights_json[\"summary\"],\n        \", \".join(insights_json[\"entities\"]),\n        \"\\n\".join(insights_json[\"actions\"]),\n        sentiment_json\n    )\n\ndef run_dash_with_ngrok():\n    port = 8050\n    public_url = ngrok.connect(port)\n    print(\"Open this link in your browser:\", public_url)\n\n    @app.callback(\n        Output(\"ngrok-link\", \"children\"),\n        Input(\"upload-audio\", \"contents\")\n    )\n    def show_ngrok_link(_):\n        return html.A(public_url, href=public_url, target=\"_blank\")\n\n    app.run(port=port, debug=False)\n\nthread = Thread(target=run_dash_with_ngrok)\nthread.start()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}